## Ranking Problem {#sec:ranking-problem}

In the problem of estimating ranks of several unknown real-valued parameters $\theta_1,\theta_2,\ldots, \theta_K$, it is desired to rank these $K$ parameters from smallest to largest, $\theta_{(1)}<\theta_{(2)}<\ldots<\theta_{(K)}$. Let $r_1,r_2,\ldots,r_K$ be the true unknown ranks of $\theta_1,\theta_2,\ldots, \theta_K$. For example, if $\theta_{(2)}=\theta_4$, then $r_4=2$. Our first goal is to derive a joint confidence region for $\theta_1,\theta_2,\ldots, \theta_K$ and thereby obtain joint confidence intervals for the $r_1,r_2,\ldots,r_K$ using a result from @klein. In deriving a joint confidence region for $\theta_1,\theta_2,\ldots,\theta_K$, we will make use of the corresponding point estimates $\hat{\theta}_1,\hat{\theta}_2,\ldots,\hat{\theta}_K$. 

There are several real-life applications of this problem. For example, the study of @klein illustrates the problem of ranking the US states in terms of travel time to work. In this context, $\theta_k$ is the true mean travel time to work for state $k$, and we wish to find a joint confidence statement about $r_1,r_2,\ldots,r_K$. Our second goal is to derive a joint confidence region for the ordered parameters $\theta_{(1)},\theta_{(2)},\ldots,\theta_{(K)}$. In the context of senatorial elections where $\theta_k$ denotes the proportion of votes earned by a candidate, confidence intervals on ordered parameters help determine if one candidate's lead is statistically significant. If the confidence intervals for two candidates' support levels do not overlap, it is highly likely (at the specified confidence level) that the candidate with the higher point estimate is the true winner. Overall uncertainty also highlights that in election polls, reported candidate rankings that seem decisive may in fact be much less stable once uncertainty is considered, and hence it is possible that these rankings do not align with the actual election outcomes. This is particularly important when discussing the potential influence of such polls on voter behavior---commonly referred to as the bandwagon effect---because apparent leads may encourage support for candidates who are not actually ahead, underscoring the value of clearly communicating the uncertainty in poll-based rankings. As to @carling, in institutions like hospitals that are ranked according to mortality rates for a particular ailment, failure to report uncertainties may create a misleading impression, potentially causing fear among patients and casting a negative light on hospitals that appear at the bottom of the ranking.

A mathematical definition of $r_k$ is as follows:

\begin{equation}
  r_k = \sum^K_{j=1} I(\theta_j \leq \theta_k) = 1 + \sum_{j:j \neq k} I(\theta_j \leq \theta_k), \qquad \text{for} \; k = 1, 2, \dots, K.
  \label{eq:rank1}
\end{equation}

Let $\hat{r}_k$ denote the estimated rank of the $k$th observation. This can be computed determined based on 

\begin{equation}
  \hat{r}_k = 1 + \sum_{j:j \neq k} I(\hat{\theta}_j \leq \hat{\theta}_k), \qquad \text{for} \; k = 1, 2, \dots, K.
  \label{eq:rank2}
\end{equation}

In this chapter, we review the various approaches to solve this problem of finding a joint confidence region for $r_1, r_2,\ldots,r_K$.

## Klein's Joint Confidence Region for Overall Ranking Uncertainty {#sec:kleins-joint-confidence-region-for-overall-ranking-uncertainty}

The study of @klein assumes that $\hat{\theta}_k\sim N(\theta_k,\sigma^2_k), k=1,2,\ldots,K$ where $\theta_k$ is unknown but $\sigma^2_k$ is known.  Their solution hinges on the fact that the uncertainty in the ranks is determined by the uncertainty in the parameters (@mogstadt23). We now describe their methodology.

Suppose that for each \(k \in \left\{1, 2, \dots, K\right\}\) there exists values $L_k$ and $U_k$ such that 

\begin{equation}
  \theta_k \in \left( L_k, U_k \right), k=1,2,\ldots,K.
  \label{eq:theta_int}
\end{equation}
Moreover, define the quantities $I_k$, $\Lambda_{Lk}$, $\Lambda_{Rk}$, $\Lambda_{Ok}$ as follows:

\begin{equation}
    \left.
        \begin{array}{cc}
                I_k = \left\{ 1, 2, \dots, K \right\} - \left\{k \right\}, \\
                \Lambda_{Lk} = \left\{ j \in I_k : U_j \leq L_k \right\}, \\
                \Lambda_{Rk} = \left\{ j \in I_k : U_k \leq L_j \right\}, \\
                \Lambda_{Ok} = \left\{ j \in I_k:U_j > L_k \ \text{and} \ U_k > L_j \right\} = I_k - \left\{ \Lambda_{Lk} \cup \Lambda_{Rk} \right\}
    	\end{array}
    \right\}
  \notag
\end{equation}
which implies the following:

1. $j \in \Lambda_{Lk} \leftrightarrow \left(L_j, U_j\right) \cap \left(L_k, U_k\right) = \emptyset$ and $\left(L_j, U_j\right)$ lies to the left of $\left(L_k, U_k\right)$;
2. $j \in \Lambda_{Rk} \leftrightarrow \left(L_j, U_j\right) \cap \left(L_k, U_k\right) = \emptyset$ and $\left(L_j, U_j\right)$ lies to the right of $\left(L_k, U_k\right)$;
3. $j \in \Lambda_{Ok} \leftrightarrow \left(L_j, U_j\right) \cap \left(L_k, U_k\right) \neq \emptyset$
4. $\Lambda_{Lk}, \Lambda_{Rk},$ and $\Lambda_{Ok}$ are mutually exclusive, and $\Lambda_{Lk} \cup \Lambda_{Rk} \cup \Lambda_{Ok} = I_k$  

Let $\lvert A \rvert$ denote the number of elements in a set $A$. If the condition in (\ref{eq:theta_int}) holds, the main result from @klein gives a range for the value of $r_k$ for each \(k \in \left\{1, 2, \dots, K\right\}\) as follows:

\begin{equation}
  r_k \in 
  \left\{ 
  \lvert \Lambda_{Lk} \rvert + 1,  
  \lvert \Lambda_{Lk} \rvert + 2,
  \lvert \Lambda_{Lk} \rvert + 3,
  \dots,
  \lvert \Lambda_{Lk} \rvert + \lvert \Lambda_{Ok} \rvert + 1
  \right\}
  \label{eq:klein_jcs}
\end{equation}
Note that the number of elements in the range given in (\ref{eq:klein_jcs}) is $|\Lambda_{Ok}|+1$. Since the smaller difference between \(U_k\) and \(L_k\) leads to a smaller \(\lvert \Lambda_{Ok} \rvert\), narrower confidence intervals for $\theta_1,\theta_2,\ldots,\theta_K$ are desirable.

Suppose that for random quantities $L_k$ and $U_k$ the event defined in (\ref{eq:theta_int}) satisfies the following probability condition:

\begin{equation}
  P\left[ \bigcap^K_{k=1} \left\{ \theta_k \in \left(L_k, U_k\right) \right\} \right] \geq 1-\alpha,
  \label{eq:joint_cov1}
\end{equation}
then, by the result of @klein, it also follows that 

\begin{equation}
  P\left[
  \bigcap^K_{k=1}
  \left\{
  r_k \in 
  \left\{ 
  \lvert \Lambda_{Lk} \rvert + 1,  
  \lvert \Lambda_{Lk} \rvert + 2,
  \lvert \Lambda_{Lk} \rvert + 3,
  \dots,
  \lvert \Lambda_{Lk} \rvert + \lvert \Lambda_{Ok} \rvert + 1
  \right\}
  \right\}
  \right] \geq 1-\alpha.
  \label{eq:joint_cov2}
\end{equation}

Due to the assumption of normality on $\hat{\theta}_k$ as well as the fact that $\sigma_k$ is assumed known, @klein set the confidence intervals $(L_k,U_k)$ for $\theta_k$ to be of the form $\hat \theta_k \pm t \times \sigma_k$ for $k \in \left\{1, 2, \dots, K\right\}$. One may use the Bonferroni approach to choose $t$. If such an approach is used, the choice of $t$ that would satisfy (\ref{eq:joint_cov1}) is $t = z_{\alpha/2K}$. Another choice of $t$ is one that exploits the independence assumption on $\hat{\theta}_k$. Such a choice is given by $z_{\gamma/2}$ where $\gamma=1-(1-\alpha)^{1/K}$. To see why this is the case, note that if we define $Y_k$ as follows:

\begin{align}
Y_k = \frac{\left( \hat{\theta}_k - \theta_k\right)}{\sigma_k} &\sim N\!(0, 1), \quad k =1,2, \dots, K \label{eq:standardized} 
\end{align}
then we get

\begin{align}
\prod^K_{k=1} P\left( \lvert Y_k \rvert \leq t\right) &=
\prod^K_{k=1} P\left( \hat{\theta}_k-t \cdot \sigma_k \leq  \theta_k \leq  \hat{\theta}_k + t \cdot \sigma_k \right) \notag \\
&=\prod^K_{k=1} 1-[1-(1-\alpha)^{1/K}] \\
&=1-\alpha. \notag
\end{align}

## Alternative Approaches for Ranking Uncertainty

While Kleinâ€™s approach provides one framework for constructing joint confidence regions for ranks, several other studies have explored related problems using different formulations or assumptions. These alternative methods vary in whether they account for dependence structures, rely on model-based estimation, or use resampling techniques such as the bootstrap.

### Calculated Measure

Other studies with similar concern include that of @carling who suggest the use of a statistic $C$ which quantifies the number of positions ranked populations would on average change their order due to random variation. They calculate the measure using a bootstrap approach. Working on risk ratios $p_k$ of $K$ units, @carling draw $B$ bootstrap proportions $\hat{p}^*_k$ from (\ref{eq:carling_bs}):

\begin{equation}
\hat{p}^*_{bk} \sim  N \left(\hat{p}_k, \frac{\hat{p}_k(1-\hat{p}_k)}{n_k} \right), \quad k = 1,2, \dots, K; \; b = 1,2, \dots, B
  \label{eq:carling_bs}
\end{equation}
and $\hat{p}_k$ and $n_k$ are the sample proportion and sample size, respectively, corresponding to the $k$th unit. For each bootstrap iteration $b$, they sort the set $\hat{p}^*_{b1},\ldots,\hat{p}^*_{bK}$ to get the corresponding ranks $r^*_{bk}, k=1,2,\ldots,K$. The difference $d_{bk}$ between the original and bootstrap rank is then calculated as $d_{bk} =$ \(\lvert \hat{r}_k - \hat{r}^*_{bk}\rvert\), where $\hat{r}_k$ is the rank of $\hat{p}_k$ in the set $\{\hat{p}_1,\hat{p}_2,\ldots,\hat{p}_K\}$. In turn, $\bar{d}_k$ is obtained by taking the average of $d_{bk}$ across the bootstrap samples. Finally, the overall measure $C$ is calculated as the average of $\bar{d}_k$ across all $K$ units. 

### Pairwise Difference {#sec:pairwise}

@jelle, requiring only the mean estimates and their corresponding standard errors similar to @klein, apply Tukey's Honest Significant Difference (HSD) to test $H_0: \theta_j - \theta_{k} = 0$ for all $j \neq k \in {1, ..., K}$ at level $\alpha$. Tukey's HSD is typically used to provide simultaneous confidence statements about the differences between the means while controlling the family-wise error rate (FWER). @jelle come up with a joint confidence set for ranks expressed as

\begin{align}
  \left(
  1 + \#\left\{ j: \frac{\hat{\theta}_j - \hat{\theta}_{k}}{\sqrt{\sigma^2_j + \sigma^2_{k}} }> q_{1-\alpha} \right\},
  \;
  K - \#\left\{ j: \frac{\hat{\theta}_j - \hat{\theta}_{k}}{\sqrt{\sigma^2_j + \sigma^2_{k}} } < -q_{1-\alpha} \right\}
  \right), \label{eq:jelle} \\
  \qquad
  \text{for}\;k=1,2,\dots,K \notag
\end{align}
where $q_{1-\alpha}$ is the $(1-\alpha)$ quantile of the distribution of the studentized range, 
\begin{equation}
\underset{j,k=1, \dots K}{\max} \frac{\lvert \theta_j-\theta_{k}\rvert}{\sqrt{\sigma^2_j + \sigma^2_{k}}}. \notag
\end{equation}
In (\ref{eq:jelle}), the notation $\#\{\cdot\}$ counts the number of pairwise hypotheses that are rejected according to Tukey's HSD, which determines the lower and upper bounds of the confidence interval for the rank of $\hat{\theta}_k$. Their method has simultaneous coverage of at least $1-\alpha$ and exactly $1-\alpha$ when $\theta_1=\theta_2=\cdots=\theta_K$. However, their approach tends to be overly conservative, showing coverage levels between 0.996 and 1.0 at a 0.90 nominal level in simulations, when the $\theta$s differ. They also demonstrated that as the true differences increase from 0 to 0.5, the coverage quickly increases from the nominal level to 1. As a remedy, they proposed a rescaling technique that brings the coverage closer to the nominal level, though it remains conservative (e.g., from 1.0 to 0.978, from 0.998 to 0.961---at 0.90 confidence level).

@mogstadt23 presents another technique that closely resembles the procedure by @klein and @jelle. However, they define ranks in the opposite way (i.e., larger rank value for lower estimate). They construct the rectangular confidence region in (\ref{eq:mogstadt23_ci}), from the pairwise differences of the estimators $\hat{\theta}_1, \dots, \hat{\theta}_K$ and an estimator of the variance of $\hat{\theta}_{j}-\hat{\theta}_{k}$. The quantities $\hat{\theta}_1, \dots, \hat{\theta}_K$ need not be independent. Let $P_k$ be the distribution from which $\hat{\theta}_k$ is estimated; and let $\hat{P}_k$ denote the estimate of $P_k$.  The joint confidence intervals for $r_1,\dots,r_K$ are obtained from the joint confidence intervals of the pairwise difference:

\begin{equation}
\begin{split}
C(1-\alpha, S) &= \prod_{(j,k) \in S} \left[ 
\hat{\theta}_j - \hat{\theta}_{k} \pm \sqrt{\sigma_j + \sigma_k} \;L^{-1}(1-\alpha, S, \hat{P})
\right]
, \\
&\qquad\qquad\qquad\ S \subseteq \{(j,k) \in K\times K: j\neq k\}
\end{split}
\label{eq:mogstadt23_ci}
\end{equation}
where $L^{-1}(1-\alpha,S,P)$ is the $(1-\alpha)$-quantile corresponding to the CDF:

\begin{equation}
  L(x, S, P) = P\left\{ \underset{(j,k)\in S}{\max}
  \frac{\lvert
  \hat{\theta}_j - \hat{\theta}_{k} - (\theta_j-\theta_k)
  \rvert}{\sqrt{\sigma^2_j + \sigma^2_k}} \leq x
  \right\}.
  \label{eq:mogstadt23_quantile}
\end{equation}

They added that if the estimators $\hat{\theta}_1, \hat{\theta}_2, \dots, \hat{\theta}_K$ are jointly asymptotically normally distributed, then the quantiles $L^{-1}(1-\alpha, S, \hat{P})$, can be computed from the limiting distributions of the max-statistics shown in (\ref{eq:mogstadt23_quantile}), through resampling methods. @mogstadt23 show that their approach generally leads to a confidence set that is narrower than that of @klein.

### Accounting for Data Dependencies

Some approaches explicitly account for dependencies in the data. @spiegel use multilevel models, in the context of ranking education and health institutions (e.g., schools, hospitals, medical practitioners, etc.), to address the hierarchical nature of data structures associated with institutional performance. Rank uncertainty is presented through a visualization in which non-overlap of confidence intervals conveyed a significant difference between compared institutions (Goldstein \& Healy, 1995). In an alternative approach, along with institution effect estimation through Gibbs sampling, the rank is obtained for each iteration. Their example illustrates that while the multilevel model made individual estimates more accurate, it also had the effect of making the ranks even more uncertain.

@zhang analyze U.S. age-adjusted cancer incidence and mortality rates across states and counties by computing individual and overall simultaneous confidence intervals for age-adjusted health index using the Monte Carlo method. Because many health conditions are age-dependent, they use age-adjusted rates to minimize the confounding effect of age differences when comparing different population groups. They also extend their method to handle cases where only the adjusted rates and confidence intervals are available, aligning it more closely with the approach of @klein. @jelle show their technique to result in joint confidence sets with very low coverage probabilities and which are only able to reach the nominal level when differences among the means are large enough.

@miller mention that in some use cases such as institutions ranking, dependencies can be accommodated through conditioning, similar to the above approaches. However, in genomics where data on expression levels of different genes from the same individual are generally not independent, they suggest using an "independent component" version of the bootstrap on the sample, where m-out-of-n bootstrap (m < n) is applied as though the ranked variables were statistically independent. They show this to perform at its best when a reasonable level of correlation is present among the variables.

@mogstadt25, in their recent study, tackle the ranking of political candidates or parties using the estimated share of support each one receives in surveys. They use the multinomial distribution to develop confidence sets for finite samples and explore the use of the bootstrap in the case of approximately large samples. They address the dependence attributed to the success probabilities of different categories by using their proposed bootstrap algorithm. Their simulations show that bootstrap-based confidence sets may have coverage probability below the nominal level despite their being excessively wide. In contrast, the finite-sample confidence sets have coverage probability at least as large as expected and may even be relatively shorter.




